# Pipeline de Engenharia de Dados com Kafka, Spark e AWS

## üìå Vis√£o Geral
Este projeto foi desenvolvido como um desafio pr√°tico de Engenharia de Dados, com o objetivo de construir uma **pipeline de dados ponta a ponta**, integrando ingest√£o, processamento e armazenamento em ambiente distribu√≠do.

A solu√ß√£o utiliza **Apache Kafka** para streaming de dados, **Apache Spark** para processamento e transforma√ß√£o, e **Amazon S3** como Data Lake, seguindo a arquitetura **Bronze, Silver e Gold**.

O projeto foi executado em ambiente containerizado com **Docker**, simulando um cen√°rio real de produ√ß√£o.

---

## üéØ Objetivos do Projeto
- Implementar ingest√£o de dados em tempo real
- Processar dados utilizando Spark
- Organizar dados em camadas Bronze, Silver e Gold
- Armazenar dados em Data Lake (Amazon S3)
- Integrar m√∫ltiplas tecnologias em uma arquitetura escal√°vel
- Demonstrar dom√≠nio de ferramentas de Engenharia de Dados

---

## üõ†Ô∏è Tecnologias Utilizadas
- **Apache Kafka**
- **Kafka Connect (Source e Sink)**
- **Apache Spark**
- **PostgreSQL**
- **Amazon S3**
- **Docker e Docker Compose**
- **Python**
- **Arquitetura Bronze / Silver / Gold**

---

## üß± Arquitetura da Solu√ß√£o
A arquitetura foi projetada para suportar ingest√£o cont√≠nua e processamento distribu√≠do de dados.

Fluxo resumido:
1. Dados transacionais armazenados em PostgreSQL
2. Kafka Connect realiza a ingest√£o dos dados
3. Apache Kafka atua como camada de streaming
4. Apache Spark consome os dados do Kafka
5. Dados s√£o transformados e organizados nas camadas Bronze, Silver e Gold
6. Armazenamento final no Amazon S3 (Data Lake)

![Arquitetura da Solu√ß√£o](images/arquitetura_solucao.png)

---

## üóÉÔ∏è Arquitetura Bronze, Silver e Gold
A organiza√ß√£o dos dados segue o padr√£o de Data Lake moderno:

- **Bronze:** dados brutos ingeridos do Kafka
- **Silver:** dados tratados, limpos e padronizados
- **Gold:** dados prontos para consumo anal√≠tico

![Bronze Silver Gold](images/bronze_silver_gold.png)

---

## üîÑ Streaming de Dados com Apache Kafka
O Apache Kafka √© utilizado como plataforma central de streaming, permitindo ingest√£o cont√≠nua e desacoplamento entre produtores e consumidores.

Principais pontos:
- Uso de t√≥picos para organiza√ß√£o dos dados
- Kafka Connect para integra√ß√£o com PostgreSQL e S3
- Alta escalabilidade e toler√¢ncia a falhas

![Pipeline Kafka](images/kafka_pipeline.png)

---

## ‚öôÔ∏è Processamento de Dados com Apache Spark
O Apache Spark √© respons√°vel pelo processamento e transforma√ß√£o dos dados, utilizando:
- DataFrames
- Spark SQL
- Processamento distribu√≠do

As transforma√ß√µes aplicadas garantem qualidade, padroniza√ß√£o e prepara√ß√£o dos dados para an√°lises posteriores.

---

## ‚òÅÔ∏è Armazenamento em Data Lake (AWS S3)
O Amazon S3 √© utilizado como Data Lake, oferecendo:
- Escalabilidade
- Alta durabilidade
- Baixo custo de armazenamento
- Integra√ß√£o com ferramentas anal√≠ticas

---

## üìÇ Documenta√ß√£o do Projeto
- [Contexto do Desafio](docs/01-contexto-do-desafio.md)
- [Arquitetura da Solu√ß√£o](docs/02-arquitetura-da-solucao.md)
- [Pipeline Bronze, Silver e Gold](docs/03-pipeline-bronze-silver-gold.md)
- [Streaming com Kafka](docs/04-streaming-com-kafka.md)
- [Processamento com Spark](docs/05-processamento-com-spark.md)
- [Implanta√ß√£o e Execu√ß√£o](docs/06-implantacao-e-execucao.md)

---

## ‚úÖ Considera√ß√µes Finais
Este projeto demonstra a constru√ß√£o de uma pipeline de Engenharia de Dados moderna, integrando tecnologias amplamente utilizadas no mercado e aplicando boas pr√°ticas de arquitetura, escalabilidade e processamento distribu√≠do.

O reposit√≥rio tem como objetivo servir como **portf√≥lio t√©cnico**, evidenciando o uso pr√°tico das ferramentas e a integra√ß√£o entre elas.
